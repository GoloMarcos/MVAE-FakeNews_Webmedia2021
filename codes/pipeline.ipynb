{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzZtmgX7QLLZ"
      },
      "source": [
        "#Bases de Dados Textuais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jvgHfW6n-xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd718681-b9f6-41c4-9263-e99ae9b2ba67"
      },
      "source": [
        "!pip install git+https://github.com/GoloMarcos/FKTC/\n",
        "\n",
        "from FakeNewsTextCollections import datasets\n",
        "\n",
        "bases = datasets.load('datasets/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/GoloMarcos/FKTC/\n",
            "  Cloning https://github.com/GoloMarcos/FKTC/ to /tmp/pip-req-build-4un2y3tt\n",
            "  Running command git clone -q https://github.com/GoloMarcos/FKTC/ /tmp/pip-req-build-4un2y3tt\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from FakeNewsTextCollections==0.1.0) (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown->FakeNewsTextCollections==0.1.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown->FakeNewsTextCollections==0.1.0) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown->FakeNewsTextCollections==0.1.0) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown->FakeNewsTextCollections==0.1.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown->FakeNewsTextCollections==0.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown->FakeNewsTextCollections==0.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown->FakeNewsTextCollections==0.1.0) (1.24.3)\n",
            "Building wheels for collected packages: FakeNewsTextCollections\n",
            "  Building wheel for FakeNewsTextCollections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for FakeNewsTextCollections: filename=FakeNewsTextCollections-0.1.0-cp37-none-any.whl size=2058 sha256=b9b562e1de758869af31eee91e4c122bd67a2722cf3dd60e7035292a7845330f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-52_dul7a/wheels/16/c3/42/3e4f9da2d68ba20a9da361eec2ae670a4e48d4794e00ca647d\n",
            "Successfully built FakeNewsTextCollections\n",
            "Installing collected packages: FakeNewsTextCollections\n",
            "Successfully installed FakeNewsTextCollections-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdJVdMWn1z-M"
      },
      "source": [
        "# Train and Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvnAFUgW14Io"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_test_split_pipeline(df_int, percent,column, f1, f2, f3, with_name, parcial_name=''):\n",
        "\n",
        "  name = parcial_name + '_' + str(percent) + '_' + str(f1)\n",
        "\n",
        "  if percent == 3 or percent == 5 or percent == 7:\n",
        "    df_test = np.array(df_int[df_int.fold != f1][column].to_list()) \n",
        "    df_fold = np.array(df_int[df_int.fold == f1][column].to_list())\n",
        "    train_percent = percent/10\n",
        "    test_percent = 1 - train_percent\n",
        "    df_train, discard = train_test_split(df_fold, test_size=test_percent, random_state=42)\n",
        "    \n",
        "  elif percent == 10:\n",
        "    df_train = np.array(df_int[df_int.fold == f1][column].to_list())\n",
        "    df_test = np.array(df_int[df_int.fold != f1][column].to_list())\n",
        "\n",
        "  elif percent == 20:\n",
        "    df_train = np.array(df_int[(df_int.fold == f1) | (df_int.fold == f2)][column].to_list())\n",
        "    df_test = np.array(df_int[(df_int.fold != f1) & (df_int.fold != f2)][column].to_list())\n",
        "    name += '-' + str(f2)\n",
        "\n",
        "  elif percent == 30:\n",
        "    df_train = np.array(df_int[(df_int.fold == f1) | (df_int.fold == f2) | (df_int.fold == f3)][column].to_list())\n",
        "    df_test = np.array(df_int[(df_int.fold != f1) & (df_int.fold != f2) & (df_int.fold != f3)][column].to_list())\n",
        "    name += '-' + str(f2) + '-' + str(f3)\n",
        "\n",
        "  if with_name:\n",
        "    return df_train, df_test, name\n",
        "  else:\n",
        "    return df_train, df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjTSXOYkawF1"
      },
      "source": [
        "# One Class Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktS6yET_2W2O"
      },
      "source": [
        "def init_metrics():\n",
        "  metricas = {\n",
        "    'precisao' : [],\n",
        "    'revocacao' : [],\n",
        "    'f1-score' : [],\n",
        "    'auc_roc' : [],\n",
        "    'acuracia' : [],\n",
        "    'tempo' : []\n",
        "  }\n",
        "\n",
        "  return metricas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpMUzHWk3HnL"
      },
      "source": [
        "def save_values(metricas, values):\n",
        "  metricas['precisao'].append(values['precisão'])\n",
        "  metricas['revocacao'].append(values['revocação'])\n",
        "  metricas['f1-score'].append(values['f1-score'])\n",
        "  metricas['auc_roc'].append(values['auc_roc'])\n",
        "  metricas['acuracia'].append(values['acurácia'])\n",
        "  metricas['tempo'].append(values['tempo'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvF4n-Jxaz4X"
      },
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from gc import collect\n",
        "\n",
        "\n",
        "def evaluation_one_class(preds_interest, preds_outliers):\n",
        "  y_true = [1]*len(preds_interest) + [-1]*len(preds_outliers)\n",
        "  y_pred = list(preds_interest)+list(preds_outliers)\n",
        "  return classification_report(y_true, y_pred, output_dict=True)\n",
        "\n",
        "def evaluate_model(X_train, X_test, X_outlier, model):\n",
        "  \n",
        "  one_class_classifier = model.fit(X_train)\n",
        "\n",
        "  Y_pred_interest = one_class_classifier.predict(X_test)\n",
        "  \n",
        "  Y_pred_ruido = one_class_classifier.predict(X_outlier)\n",
        "\n",
        "  score_interest = one_class_classifier.decision_function(X_test)\n",
        "\n",
        "  score_outlier = one_class_classifier.decision_function(X_outlier)\n",
        "\n",
        "  y_true = np.array([1] * len(X_test) + [-1] * len(X_outlier))\n",
        "\n",
        "  fpr, tpr, limiares = roc_curve(y_true, np.concatenate([score_interest,score_outlier]))\n",
        "\n",
        "  dic = evaluation_one_class(Y_pred_interest, Y_pred_ruido)\n",
        "\n",
        "  metricas = {}\n",
        "  metricas['precisão'] = dic['1']['precision']\n",
        "  metricas['revocação'] = dic['1']['recall']\n",
        "  metricas['f1-score'] = dic['1']['f1-score']\n",
        "  metricas['auc_roc'] = roc_auc_score(y_true, np.concatenate([score_interest,score_outlier]))\n",
        "  metricas['acurácia'] = dic['accuracy']\n",
        "\n",
        "  return metricas,fpr,tpr\n",
        "\n",
        "def escreve(metricas, file_name, line_parameters, folds, caminho):\n",
        "  \n",
        "  path = caminho\n",
        "  if not Path(path + file_name).is_file():\n",
        "    arquivo = open(path + file_name,'w')\n",
        "    string = 'Parametros'\n",
        "    \n",
        "    for metrica in metricas.keys():\n",
        "      string+= ';' + metrica + '-mean;' + metrica + '-std'\n",
        "    string+= '\\n'\n",
        "    \n",
        "    arquivo.write(string)\n",
        "    arquivo.close()\n",
        "  \n",
        "  arquivo = open(path + file_name,'a')\n",
        "  string = line_parameters\n",
        "\n",
        "  for metrica in metricas.keys():\n",
        "    string += ';' + str(np.mean(metricas[metrica])) + ';' + str(np.std(metricas[metrica])) \n",
        "    \n",
        "  string+= '\\n'\n",
        "  arquivo.write(string)\n",
        "  arquivo.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmYeJaxfuRMU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def save_roc_curve(fpr,tpr,auc_roc, name):\n",
        "  path = ''\n",
        "  name += '.pdf'\n",
        "  fig = plt.figure()\n",
        "  plt.plot(fpr, tpr, label='AUC ROC = %0.4f' % auc_roc)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('Taxa de Falso Positivo (TFP)')\n",
        "  plt.ylabel('Taxa de Verdadeiro Positivo (TVP)')\n",
        "  plt.title('Curva ROC')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.savefig(path+name)   \n",
        "  plt.close(fig)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF-M_INrveAH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def plot_TSNE(X_int, X_outlier, name):\n",
        "  \n",
        "  path = '/content/drive/MyDrive/USP/projeto/artigos/FakeNews/TSNEs/'\n",
        "  name += '.pdf'\n",
        "\n",
        "  X = np.concatenate((X_outlier,X_int))\n",
        "\n",
        "  X_embedded = TSNE(n_components=2).fit_transform(X)\n",
        "\n",
        "  df1 = pd.DataFrame(X_embedded)\n",
        "  df1['Classe'] = np.array(['Não Interesse'] * len(X_outlier) + ['Interesse'] * len(X_int))\n",
        "  df1 = df1.dropna()\n",
        "\n",
        "  g = sns.scatterplot(x=0,y=1,data=df1, hue=\"Classe\", legend=True)\n",
        "  g.set(xlabel=None)\n",
        "  g.set(ylabel=None)\n",
        "  plt.savefig(path+name)\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qyMakW-Hf3Y"
      },
      "source": [
        "# One Clas SVDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIcP5JeAADkQ"
      },
      "source": [
        "importar sklearn normal antes de rodar esses trechos de código para o sklearn como o SVDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoycqBNKtHcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd47c25-a419-4d24-a81d-376f2c39ae76"
      },
      "source": [
        "!gdown --id 1HfSoQ7SSsdmIVzVk1qqWyoP_URR4a5Bi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HfSoQ7SSsdmIVzVk1qqWyoP_URR4a5Bi\n",
            "To: /content/scikit-learn.tar.gz\n",
            "178MB [00:03, 48.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CQKxP-MwR4C"
      },
      "source": [
        "!tar -xzvf scikit-learn.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIhCEdbPIBVI",
        "outputId": "cba48a55-243c-4efc-858f-a8bdd1a1da7c"
      },
      "source": [
        "cd scikit-learn/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/scikit-learn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FahfEETg7Ir2"
      },
      "source": [
        "!mv sklearn sklearn_svdd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcxNnFG-sFUR"
      },
      "source": [
        "from sklearn_svdd.svm import SVDD\n",
        "modelos = {\n",
        "    'SVDD_RBF_0.001_scale' : SVDD(kernel='rbf', nu = 0.001, gamma='scale'),\n",
        "    'SVDD_RBF_0.01_scale' : SVDD(kernel='rbf', nu = 0.01, gamma='scale'),\n",
        "    'SVDD_RBF_0.05_scale' : SVDD(kernel='rbf', nu=0.05, gamma='scale'),\n",
        "    'SVDD_RBF_0.1_scale' : SVDD(kernel='rbf', nu = 0.1, gamma='scale'),\n",
        "    'SVDD_RBF_0.2_scale' : SVDD(kernel='rbf', nu = 0.2, gamma='scale'),\n",
        "    'SVDD_RBF_0.3_scale' : SVDD(kernel='rbf', nu = 0.3, gamma='scale'),\n",
        "    'SVDD_RBF_0.4_scale' : SVDD(kernel='rbf', nu = 0.4, gamma='scale'),\n",
        "    'SVDD_RBF_0.5_scale' : SVDD(kernel='rbf', nu = 0.5, gamma='scale'),\n",
        "    'SVDD_RBF_0.6_scale' : SVDD(kernel='rbf', nu = 0.6, gamma='scale'),\n",
        "    'SVDD_RBF_0.7_scale' : SVDD(kernel='rbf', nu = 0.7, gamma='scale'),\n",
        "    'SVDD_RBF_0.8_scale' : SVDD(kernel='rbf', nu = 0.8, gamma='scale'),\n",
        "    'SVDD_RBF_0.9_scale' : SVDD(kernel='rbf', nu = 0.9, gamma='scale')\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0YmCVtQfPnH",
        "outputId": "bc582d86-d4f0-43e5-facb-4e911ac184ff"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvI8XJz5VBA9"
      },
      "source": [
        "# Bag-of-Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA0G-kf1coxs"
      },
      "source": [
        "## Code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhcDoCqfVFiV"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk.stem\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def hasNumbers(inputString):\n",
        " return any(char.isdigit() for char in inputString)\n",
        "\n",
        "class MyTokenizer:\n",
        "    def __init__(self, language):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "        if language == 'english':\n",
        "          self.STOPWORDS = nltk.corpus.stopwords.words('english')\n",
        "          self.stemmer = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "        if language == 'spanish':\n",
        "          self.STOPWORDS = nltk.corpus.stopwords.words('spanish')\n",
        "          self.stemmer = nltk.stem.SnowballStemmer('spanish')\n",
        "\n",
        "        if language == 'portuguese':\n",
        "          self.STOPWORDS = nltk.corpus.stopwords.words('portuguese')\n",
        "          self.stemmer = nltk.stem.SnowballStemmer('portuguese')\n",
        "\n",
        "        if language == 'multilingual':\n",
        "          stop_words_pt = nltk.corpus.stopwords.words('portuguese')\n",
        "          stop_words_es = nltk.corpus.stopwords.words('spanish')\n",
        "          stop_words_en = nltk.corpus.stopwords.words('english')\n",
        "          self.STOPWORDS = set(nltk.corpus.stopwords.words('spanish')).union(set(nltk.corpus.stopwords.words('portuguese'))).union(set(nltk.corpus.stopwords.words('english')))\n",
        "          self.stemmer = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        L1 = [t for t in word_tokenize(doc)]\n",
        "        L2 = []\n",
        "        for token in L1:\n",
        "          if token not in self.STOPWORDS and token.isnumeric()==False and len(token) > 2 and hasNumbers(token)==False: L2.append(token)\n",
        "        L3 = [self.stemmer.stem(self.wnl.lemmatize(t)) for t in L2]\n",
        "        return L3\n",
        "        \n",
        "def tipo_peso_termo(tipo_bow):\n",
        "  if tipo_bow == 'term-frequency':\n",
        "    vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', ngram_range=(1,1), min_df=1, tokenizer=MyTokenizer('multilingual'))\n",
        "  if tipo_bow == 'term-frequency-IDF':\n",
        "    vectorizer = TfidfVectorizer(strip_accents='ascii', stop_words='english', ngram_range=(1,1), min_df=1, tokenizer=MyTokenizer('multilingual'))\n",
        "  if tipo_bow == 'binary':\n",
        "    vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', ngram_range=(1,1), min_df=1, tokenizer=MyTokenizer('multilingual'), binary=True)\n",
        "  \n",
        "  return vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wPlXhMrcvDd"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTOXxpdDVgIs"
      },
      "source": [
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "lista_pesos_termo = ['term-frequency-IDF', 'term-frequency', 'binary']\n",
        "\n",
        "caminho = './resultados/'\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "for base in tqdm(bases):\n",
        "  df = bases[base]\n",
        "  print('Na base: ' + base)\n",
        "  \n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_outlier = np.array(df_out.text.to_list())\n",
        "\n",
        "  for peso_de_termo in tqdm(lista_pesos_termo):\n",
        "    print('No peso de termo: ' + peso_de_termo)\n",
        "    vectorizer = tipo_peso_termo(peso_de_termo)\n",
        "\n",
        "    for percent in tqdm(percents):\n",
        "      print('No percentual: ' + str(percent))\n",
        "\n",
        "      lista_reps = list()\n",
        "\n",
        "      for fold in tqdm(folds):\n",
        "        print('No fold ' + str(fold))\n",
        "\n",
        "        f1 = fold\n",
        "        f2 = (fold+1) % len(folds)\n",
        "        f3 = (fold+2) % len(folds)\n",
        "        \n",
        "        df_train, df_test = train_test_split_pipeline(df_int, percent,'text', f1, f2, f3, False)\n",
        "\n",
        "        vectorizer.fit(df_train)\n",
        "\n",
        "        X_train = vectorizer.transform(df_train)\n",
        "        X_test = vectorizer.transform(df_test)\n",
        "        X_outlier = vectorizer.transform(df_outlier)\n",
        "\n",
        "        reps = (X_train,X_test,X_outlier)\n",
        "\n",
        "        lista_reps.append(reps)\n",
        "        \n",
        "        del df_train\n",
        "        del df_test\n",
        "        del X_train\n",
        "        del X_test\n",
        "        del X_outlier\n",
        "        collect()\n",
        "\n",
        "      for modelo in tqdm(modelos):\n",
        "        print('No modelo: ' + modelo)\n",
        "        metricas = init_metrics()\n",
        "     \n",
        "        for reps in lista_reps:\n",
        "          start = time.time()\n",
        "          values,fpr,tpr = evaluate_model(reps[0].toarray(), reps[1].toarray(), reps[2].toarray(), modelos[modelo])\n",
        "          end = time.time()\n",
        "          temp = end - start\n",
        "\n",
        "          values['tempo'] = temp\n",
        "          \n",
        "          save_values(metricas,values)\n",
        "\n",
        "        nome_arquivo = base + '_BoW_' + peso_de_termo + '_' + modelo.split('_')[0] + '_' + str(percent) + '.csv'\n",
        "\n",
        "        parametros_linha = modelo\n",
        "\n",
        "        escreve(metricas, nome_arquivo, parametros_linha, len(folds), caminho)\n",
        "      \n",
        "      del lista_reps\n",
        "      collect()\n",
        "      \n",
        "  del df\n",
        "  del df_int\n",
        "  del df_out\n",
        "  del df_outlier\n",
        "  collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sho1g2R2c2hO"
      },
      "source": [
        "## TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO-AjJQX4rjM"
      },
      "source": [
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "lista_pesos_termo = ['term-frequency-IDF', 'term-frequency', 'binary']\n",
        "\n",
        "for base in bases.keys():\n",
        "  df = bases[base]\n",
        "  print('Na base: ' + base)\n",
        "  \n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_outlier = np.array(df_out.text.to_list())\n",
        "\n",
        "  for peso_de_termo in tqdm(lista_pesos_termo):\n",
        "    print('No peso de termo: ' + peso_de_termo)\n",
        "    vectorizer = tipo_peso_termo(peso_de_termo)\n",
        "\n",
        "    vectorizer.fit(df_int.text.to_list())\n",
        "\n",
        "    X_int = vectorizer.transform(df_int.text.to_list())\n",
        "    X_outlier = vectorizer.transform(df_outlier)\n",
        "    \n",
        "    name = base + '_' + peso_de_termo \n",
        "\n",
        "    plot_TSNE(X_int.toarray(), X_outlier.toarray(), name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGWVHJ7T0285"
      },
      "source": [
        "# Word-Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm8yiUxmc8Gr"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6-ii3xS05-o"
      },
      "source": [
        "caminho = './resultados/'\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "word_embeddings = ['DistilBERT Multilingua']\n",
        "\n",
        "for base in tqdm(bases):\n",
        "  df = bases[base]\n",
        "  print('Na base: ' + base)\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  for word_embedding in tqdm(word_embeddings):\n",
        "    print('Na word-embedding: ' + str(word_embedding))\n",
        "\n",
        "    df_outlier = np.array(df_out[word_embedding].to_list())\n",
        "\n",
        "    for percent in tqdm(percents):\n",
        "      print('No percentual: ' + str(percent))\n",
        "\n",
        "      lista_reps = []\n",
        "\n",
        "      for fold in tqdm(folds):\n",
        "        print('No fold ' + str(fold))\n",
        "\n",
        "        f1 = fold\n",
        "        f2 = (fold+1) % len(folds)\n",
        "        f3 = (fold+2) % len(folds)\n",
        "        \n",
        "        df_train,df_test = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3, False)\n",
        "        \n",
        "        reps = (df_train,df_test,df_outlier)\n",
        "\n",
        "        lista_reps.append(reps)\n",
        "        \n",
        "        del df_train\n",
        "        del df_test\n",
        "        collect()\n",
        "\n",
        "      for modelo in tqdm(modelos):\n",
        "        print('No modelo: ' + modelo)\n",
        "\n",
        "        metricas = init_metrics()\n",
        "\n",
        "        for reps in lista_reps:\n",
        "          start = time.time()\n",
        "          values,fpr,tpr = evaluate_model(reps[0], reps[1], reps[2], modelos[modelo])\n",
        "          end = time.time()\n",
        "          temp = end - start\n",
        "          \n",
        "          values['tempo'] = temp\n",
        "\n",
        "          save_values(metricas,values)\n",
        "\n",
        "        nome_arquivo = base +'_' + word_embedding + '_' + modelo.split('_')[0] + '_' + str(percent) + '.csv'\n",
        "\n",
        "        parametros_linha = modelo \n",
        "        \n",
        "        escreve(metricas,nome_arquivo,parametros_linha, len(folds), caminho)\n",
        "\n",
        "      del lista_reps\n",
        "      collect()\n",
        "    \n",
        "    del df_outlier\n",
        "    collect()\n",
        "\n",
        "  del df\n",
        "  del df_int\n",
        "  del df_out\n",
        "  collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF9tb9E3dhay"
      },
      "source": [
        "## TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl2REi7aha0r"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "word_embeddings = ['DistilBERT Multilingua']\n",
        "\n",
        "for base in tqdm(bases):\n",
        "  df = bases[base]\n",
        "  print('Na base: ' + base)\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_inlier = np.array(df_int['DistilBERT Multilingua'].to_list())\n",
        "  \n",
        "  df_outlier = np.array(df_out['DistilBERT Multilingua'].to_list())\n",
        "      \n",
        "  name = 'DistilBERT/tsne_' + base \n",
        "\n",
        "  plot_TSNE(df_inlier, df_outlier, name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oga_mKy9HpuU"
      },
      "source": [
        "# Density Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG8Wg3X4dHtB"
      },
      "source": [
        "## Code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgg6xx7oPeL4"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def informacao_de_densidade(lista_grupos,df_train,df_test,df_outlier):\n",
        "  L_X_train = []\n",
        "  L_X_test = []\n",
        "  L_X_outlier = []\n",
        "\n",
        "  for quantidade_de_grupos in tqdm(lista_grupos):\n",
        "    \n",
        "    kmeans = KMeans(n_clusters=quantidade_de_grupos, random_state=0).fit(df_train)\n",
        "    \n",
        "    X_train_temp = silhouette_samples(df_train, kmeans.labels_).reshape(len(df_train),1)\n",
        "    L_X_train.append(X_train_temp)\n",
        "\n",
        "    X_test_temp = silhouette_samples(df_test, kmeans.predict(df_test)).reshape(len(df_test),1)\n",
        "    L_X_test.append(X_test_temp)\n",
        "\n",
        "    X_outlier_temp = silhouette_samples(df_outlier, kmeans.predict(df_outlier)).reshape(len(df_outlier),1)\n",
        "    L_X_outlier.append(X_outlier_temp)\n",
        "\n",
        "   \n",
        "  return np.concatenate(L_X_train,axis=1), np.concatenate(L_X_test,axis=1), np.concatenate(L_X_outlier,axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB8MygdkdOL8"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gacBgINnIx9S"
      },
      "source": [
        "matriz_grupos = [[2,4,6,8,10],\n",
        "                 [3,5,7,9],\n",
        "                 [2,3,4,5,6,7,8,9,10]\n",
        "                ] \n",
        "\n",
        "caminho = './resultados/'\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "word_embeddings = ['DistilBERT Multilingua']\n",
        "  \n",
        "for base in tqdm(bases):\n",
        "  df = bases[base]\n",
        "  print('Na base: ' + base)\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  for word_embedding in tqdm(word_embeddings):\n",
        "    print('Na word-embedding: ' + str(word_embedding))\n",
        "\n",
        "    df_outlier = np.array(df_out[word_embedding].to_list())\n",
        "\n",
        "    for percent in tqdm(percents):\n",
        "      print('No percentual: ' + str(percent))\n",
        "\n",
        "      for lista_grupos in tqdm(matriz_grupos):\n",
        "        \n",
        "        lista_reps = []\n",
        "        \n",
        "        for fold in tqdm(folds):\n",
        "          print('No fold ' + str(fold))\n",
        "\n",
        "          f1 = fold\n",
        "          f2 = (fold+1) % len(folds)\n",
        "          f3 = (fold+2) % len(folds)\n",
        "\n",
        "          df_train,df_test = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3,False)\n",
        "\n",
        "          X_train,X_test,X_outlier = informacao_de_densidade(lista_grupos,df_train,df_test,df_outlier)\n",
        "\n",
        "          reps = (X_train,X_test,X_outlier)\n",
        "\n",
        "          lista_reps.append(reps)\n",
        "          \n",
        "          del df_train\n",
        "          del df_test\n",
        "          del X_train\n",
        "          del X_test\n",
        "          del X_outlier\n",
        "          collect()\n",
        "\n",
        "        for modelo in tqdm(modelos):\n",
        "          print('No modelo: ' + modelo)\n",
        "\n",
        "          metricas = init_metrics()\n",
        "\n",
        "          for reps in lista_reps:\n",
        "            start = time.time()\n",
        "            values,fpr,tpr = evaluate_model(reps[0], reps[1], reps[2], modelos[modelo])\n",
        "            end = time.time()\n",
        "            temp = end - start\n",
        "            \n",
        "            values['tempo'] = temp\n",
        "\n",
        "            save_values(metricas,values)\n",
        "\n",
        "          nome_arquivo = base + '_InfDensidade_' + word_embedding + '_' + modelo.split('_')[0] + '_' + str(percent) + '.csv'\n",
        "\n",
        "          parametros_linha = modelo + '_' + str(lista_grupos)\n",
        "\n",
        "          escreve(metricas, nome_arquivo,parametros_linha, len(folds), caminho)\n",
        "        \n",
        "        del lista_reps\n",
        "        collect()\n",
        "    \n",
        "    del df_outlier\n",
        "    collect()\n",
        "\n",
        "  del df\n",
        "  del df_int\n",
        "  del df_out\n",
        "  collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXpNUtz3dlou"
      },
      "source": [
        "## TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhlKokMqhc0p"
      },
      "source": [
        "dic = {\n",
        "    'fcn' : { 3 : [3, 5, 7, 9],\n",
        "              5 : [2, 4, 6, 8, 10],\n",
        "              7 : [3, 5, 7, 9],\n",
        "              10 : [2, 4, 6, 8, 10],\n",
        "    },\n",
        "    'fnn' : { 3 : [2, 4, 6, 8, 10],\n",
        "              5 : [2, 4, 6, 8, 10],\n",
        "              7 : [3, 5, 7, 9],\n",
        "              10 : [2, 4, 6, 8, 10],\n",
        "    },\n",
        "    'fakebr' : { 3 : [3, 5, 7, 9],\n",
        "                5 : [2, 4, 6, 8, 10],\n",
        "                7 : [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                10 : [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLYpdEsohh7N"
      },
      "source": [
        "word_embedding = 'DistilBERT Multilingua'\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "for base in tqdm(bases):\n",
        "  df = bases[base]\n",
        "  print('Na base: ' + base)\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_outlier = np.array(df_out[word_embedding].to_list())\n",
        "\n",
        "  for percent in tqdm(percents):\n",
        "    print('No percentual: ' + str(percent))\n",
        "\n",
        "    lista_grupos = dic[base][percent]\n",
        "\n",
        "    for fold in tqdm(folds):\n",
        "      print('No fold ' + str(fold))\n",
        "\n",
        "      f1 = fold\n",
        "      f2 = (fold+1) % len(folds)\n",
        "      f3 = (fold+2) % len(folds)\n",
        "\n",
        "      df_train,df_test = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3,False)\n",
        "\n",
        "      X_train,X_test,X_outlier = informacao_de_densidade(lista_grupos,df_train,df_test,df_outlier)\n",
        "\n",
        "      name = 'Densidade/' + base + '/' + str(percent) + '/tsne_fold' + str(fold)\n",
        "\n",
        "      plot_TSNE(np.concatenate((X_train,X_test)),X_outlier, name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIYYybHOqHa_"
      },
      "source": [
        "# LIWK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XCvmPM1fuIk"
      },
      "source": [
        "## Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt7GOUZvqKSR"
      },
      "source": [
        "caminho = './resultados/'\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "for base in tqdm(bases):\n",
        "  df = bases[base]\n",
        "  print('Na base: ' + base)\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_outlier = np.array(df_out['features_normalized'].to_list())\n",
        "\n",
        "  for percent in tqdm(percents):\n",
        "    print('No percentual: ' + str(percent))\n",
        "\n",
        "    lista_reps = []\n",
        "\n",
        "    for fold in tqdm(folds):\n",
        "      print('No fold ' + str(fold))\n",
        "\n",
        "      f1 = fold\n",
        "      f2 = (fold+1) % len(folds)\n",
        "      f3 = (fold+2) % len(folds)\n",
        "      \n",
        "      df_train,df_test = train_test_split_pipeline(df_int, percent,'features_normalized', f1,f2,f3,False)\n",
        "    \n",
        "      reps = (df_train,df_test,df_outlier)\n",
        "\n",
        "      lista_reps.append(reps)\n",
        "      \n",
        "      del df_train\n",
        "      del df_test\n",
        "      collect()\n",
        "    \n",
        "    for modelo in tqdm(modelos):\n",
        "      print('No modelo: ' + modelo)\n",
        "\n",
        "      metricas = init_metrics()    \n",
        "        \n",
        "      for reps in lista_reps:\n",
        "\n",
        "        start = time.time()\n",
        "        values,fpr,tpr = evaluate_model(reps[0], reps[1], reps[2], modelos[modelo])\n",
        "        end = time.time()\n",
        "        temp = end - start\n",
        "        values['tempo'] = temp\n",
        "        save_values(metricas,values)\n",
        "\n",
        "      nome_arquivo = base +'_FeaturesNormalized_' + modelo.split('_')[0] + '_' + str(percent) + '.csv'\n",
        "\n",
        "      parametros_linha = modelo \n",
        "      \n",
        "      escreve(metricas,nome_arquivo,parametros_linha, len(folds), caminho)\n",
        "    \n",
        "    del lista_reps\n",
        "    collect()  \n",
        "    \n",
        "  del df\n",
        "  del df_int\n",
        "  del df_out\n",
        "  del df_outlier\n",
        "  collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u2ps0uMgZeA"
      },
      "source": [
        "## TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHLegcZTgkhW"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "for base in tqdm(bases):\n",
        "  df = bases[base]\n",
        "  print('Na base: ' + base)\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_outlier = np.array(df_out['features_normalized'].to_list())\n",
        "\n",
        "  df_inlier = np.array(df_int['features_normalized'].to_list())\n",
        "\n",
        "  name = 'LIWK/tsne_' + base \n",
        "\n",
        "  plot_TSNE(df_inlier, df_outlier, name)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzaOvNZxsZ0g"
      },
      "source": [
        "# Autoencoder Unimodal (modalidade = Word-Embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2MBzsVqdpZ7"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtuXOmDuv8hE"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Model, load_model, save_model\n",
        "\n",
        "def autoencoder(arquitetura, len_entrada):\n",
        "\n",
        "  encoder_inputs = Input(shape=(len_entrada, ), name='Entrada_Encoder') # entrada\n",
        "  \n",
        "  if len(arquitetura) == 3:\n",
        "    camada1 = Dense(arquitetura[0], activation=\"linear\")(encoder_inputs) # camada 1\n",
        "\n",
        "    camada2 = Dense(arquitetura[1], activation=\"linear\")(camada1) # camada 2\n",
        "    \n",
        "    encoded = Dense(arquitetura[2], activation=\"linear\")(camada2) # code\n",
        "\n",
        "    camada3 = Dense(arquitetura[1], activation=\"linear\")(encoded) # camada 3\n",
        "\n",
        "    camada4 = Dense(arquitetura[0], activation=\"linear\")(camada3) # camada 4\n",
        "\n",
        "    decoder_output = Dense(len_entrada, activation=\"linear\")(camada4) # saida\n",
        "  \n",
        "  if len(arquitetura) == 2:\n",
        "    camada1 = Dense(arquitetura[0], activation=\"linear\")(encoder_inputs) # camada 1\n",
        "    \n",
        "    encoded = Dense(arquitetura[1], activation=\"linear\")(camada1) # code\n",
        "\n",
        "    camada2 = Dense(arquitetura[0], activation=\"linear\")(encoded) # camada 2\n",
        "\n",
        "    decoder_output = Dense(len_entrada, activation=\"linear\")(camada2) # saida\n",
        "  \n",
        "  if len(arquitetura) == 1:\n",
        "    encoded = Dense(arquitetura[0], activation=\"linear\")(encoder_inputs) # code\n",
        "\n",
        "    decoder_output = Dense(len_entrada, activation=\"linear\")(encoded) # saida\n",
        "\n",
        "\n",
        "  encoder = Model(encoder_inputs,  encoded)\n",
        "\n",
        "  autoencoder = Model(encoder_inputs, decoder_output)\n",
        "\n",
        "  autoencoder.compile(optimizer=tensorflow.keras.optimizers.Adam(), loss='mse')\n",
        "\n",
        "  return autoencoder, encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mkK-_SAdsh3"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrkzA-i2shJy"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "epocas = [5,10,50]\n",
        "\n",
        "arquiteturas = [[384,256],   # entrada tem tamanho do vetor da word-embedding\n",
        "                [256,128],   # aqui são definidas a segunda camada e/ou a camada codificada\n",
        "                [256]\n",
        "                ] \n",
        "\n",
        "caminho_resultado = './resultados/'\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "caminho = './modelos/AE/' # caminho para salvar os modelos de rede neurais aprendidos\n",
        "\n",
        "word_embeddings = ['DistilBERT Multilingua']\n",
        "  \n",
        "for base in tqdm(bases.keys()):\n",
        "  df = bases[base]\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  for word_embedding in tqdm(word_embeddings):\n",
        "    print('Na word-embedding: ' + str(word_embedding))\n",
        "\n",
        "    df_outlier = np.array(df_out[word_embedding].to_list())\n",
        "\n",
        "    for percent in tqdm(percents):\n",
        "      print('No percentual: ' + str(percent))\n",
        "\n",
        "      for epoca in tqdm(epocas):\n",
        "\n",
        "        for arq in tqdm(arquiteturas):\n",
        "            \n",
        "          name_arq = str(arq).replace('[', '(')\n",
        "          name_arq = name_arq.replace(']',')')\n",
        "\n",
        "          lista_reps = []\n",
        "\n",
        "          for fold in tqdm(folds):\n",
        "            print('No fold ' + str(fold))\n",
        "\n",
        "            f1 = fold\n",
        "            f2 = (fold+1) % len(folds)\n",
        "            f3 = (fold+2) % len(folds)\n",
        "\n",
        "            parcial_name = base + '_' + word_embedding + '_' + str(epoca) + '_' + str(name_arq) \n",
        "\n",
        "            df_train,df_test,nome_arq = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3,True,parcial_name)\n",
        "            \n",
        "            ae,encoder = autoencoder(arq,len(df_train[0]))\n",
        "\n",
        "            ae.fit(df_train, df_train, epochs=epoca, batch_size=32, verbose=0)\n",
        "\n",
        "            save_model(ae,caminho + nome_arq + '_autoencoder')\n",
        "            save_model(encoder,caminho + nome_arq + '_encoder')\n",
        "\n",
        "            # para carregar = \"load_model('path/to/location')\"\n",
        "\n",
        "            X_train = encoder.predict(df_train)\n",
        "            X_test = encoder.predict(df_test)\n",
        "            X_outlier = encoder.predict(df_outlier)\n",
        "\n",
        "            reps = (X_train,X_test,X_outlier)\n",
        "\n",
        "            lista_reps.append(reps)\n",
        "\n",
        "            del df_train\n",
        "            del df_test\n",
        "            del X_train\n",
        "            del X_test\n",
        "            del X_outlier\n",
        "            collect()\n",
        "              \n",
        "          for modelo in tqdm(modelos):\n",
        "            print('No modelo: ' + modelo)\n",
        "\n",
        "            metricas = init_metrics()\n",
        "            \n",
        "            for reps in lista_reps:\n",
        "              start = time.time()\n",
        "              values,fpr,tpr = evaluate_model(reps[0], reps[1], reps[2], modelos[modelo])\n",
        "              end = time.time()\n",
        "              temp = end - start\n",
        "              \n",
        "              values['tempo'] = temp\n",
        "\n",
        "              save_values(metricas,values)\n",
        "\n",
        "            nome_arquivo = base + '_Autoencoder_' + word_embedding + '_' + modelo.split('_')[0] + '_' + str(percent) + '.csv'\n",
        "\n",
        "            parametros_linha = modelo + '_' + str(epoca) + '_' + str(arq)\n",
        "\n",
        "            escreve(metricas,nome_arquivo,parametros_linha, len(folds) , caminho_resultado)\n",
        "          \n",
        "          del lista_reps\n",
        "          collect()\n",
        "\n",
        "    del df_outlier\n",
        "    collect()\n",
        "\n",
        "  del df\n",
        "  del df_int\n",
        "  del df_out\n",
        "  collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiZ8jNTldwOa"
      },
      "source": [
        "## TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGxyxEvQdypn"
      },
      "source": [
        "dic = {\n",
        "    'fcn' : { 3 : (50,[256, 128]),\n",
        "              5 : (50,[256, 128]),\n",
        "              7 : (50,[256, 128]),\n",
        "              10 : (50,[256]),\n",
        "    },\n",
        "    'fnn' : { 3 : (50,[256, 128]),\n",
        "              5 : (50,[256, 128]),\n",
        "              7 : (50,[256, 128]),\n",
        "              10 : (50,[256, 128]),\n",
        "    },\n",
        "    'fakebr' : { 3 : (50,[256, 128]),\n",
        "              5 : (50,[256, 128]),\n",
        "              7 : (50,[256, 128]),\n",
        "              10 : (50,[256, 128]),\n",
        "    },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fiH4cDKhuN5"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "word_embedding = 'DistilBERT Multilingua'\n",
        "  \n",
        "for base in tqdm(bases.keys()):\n",
        "  df = bases[base]\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_outlier = np.array(df_out[word_embedding].to_list())\n",
        "\n",
        "  for percent in tqdm(percents):\n",
        "    print('No percentual: ' + str(percent))\n",
        "\n",
        "    epoca = dic[base][percent][0]\n",
        "    arq = dic[base][percent][1]\n",
        "\n",
        "    for fold in tqdm(folds):\n",
        "      print('No fold ' + str(fold))\n",
        "\n",
        "      f1 = fold\n",
        "      f2 = (fold+1) % len(folds)\n",
        "      f3 = (fold+2) % len(folds)\n",
        "\n",
        "      df_train,df_test = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3, False)\n",
        "      \n",
        "      ae,encoder = autoencoder(arq,len(df_train[0]))\n",
        "\n",
        "      ae.fit(df_train, df_train, epochs=epoca, batch_size=32, verbose=0)\n",
        "\n",
        "      X_train = encoder.predict(df_train)\n",
        "      X_test = encoder.predict(df_test)\n",
        "      X_outlier = encoder.predict(df_outlier)\n",
        "\n",
        "      name = 'AE/' + base + '/' + str(percent) + '/tsne_fold' + str(f1) \n",
        "\n",
        "      plot_TSNE(np.concatenate((X_train,X_test)), X_outlier, name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sZkkse6cclM"
      },
      "source": [
        "# Variational Autoencoder Unimodal "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVmehV7Kd5Ji"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX9SII2BdWsz"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Model, load_model, save_model\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "  \"\"\"Uses (z_mean, z_log_var) to sample z.\"\"\"\n",
        "\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class VAE(keras.Model):\n",
        "  def __init__(self, encoder, decoder, factor_multiply,  **kwargs):\n",
        "    super(VAE, self).__init__(**kwargs)\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.factor_multiply = factor_multiply\n",
        "\n",
        "  def train_step(self, data):\n",
        "    if isinstance(data, tuple):\n",
        "      data = data[0]\n",
        "    with tf.GradientTape() as tape:\n",
        "      z_mean, z_log_var, z = encoder(data)\n",
        "      reconstruction = decoder(z)\n",
        "      reconstruction_loss = tf.reduce_mean(\n",
        "          keras.losses.mean_squared_error(data, reconstruction)\n",
        "      )\n",
        "      reconstruction_loss *= self.factor_multiply\n",
        "      kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "      kl_loss = tf.reduce_mean(kl_loss)\n",
        "      kl_loss *= -0.5\n",
        "      total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "\n",
        "    grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "    return {\n",
        "      \"loss\": total_loss,\n",
        "      \"reconstruction_loss\": reconstruction_loss,\n",
        "      \"kl_loss\": kl_loss,\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDtIMmvCfUdw"
      },
      "source": [
        "def Encoder_VAE(arquitetura,input_dim):\n",
        "\n",
        "  encoder_inputs = keras.Input(shape=(input_dim,), name='Entrada_Encoder') \n",
        "\n",
        "  if len(arquitetura) == 3:\n",
        "    camada1 = Dense(arquitetura[0], activation=\"linear\")(encoder_inputs) # camada 1\n",
        "\n",
        "    camada2 = Dense(arquitetura[1], activation=\"linear\")(camada1) # camada 2\n",
        "    \n",
        "    z_mean = layers.Dense(arquitetura[2], name=\"Z_mean\")(camada2)\n",
        "    z_log_var = layers.Dense(arquitetura[2], name=\"Z_log_var\")(camada2)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "  if len(arquitetura) == 2:\n",
        "    camada1 = Dense(arquitetura[0], activation=\"linear\")(encoder_inputs) # camada 1\n",
        "    \n",
        "    z_mean = layers.Dense(arquitetura[1], name=\"Z_mean\")(camada1)\n",
        "    z_log_var = layers.Dense(arquitetura[1], name=\"Z_log_var\")(camada1)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "  \n",
        "  if len(arquitetura) == 1:\n",
        "    z_mean = layers.Dense(arquitetura[0], name=\"Z_mean\")(encoder_inputs)\n",
        "    z_log_var = layers.Dense(arquitetura[0], name=\"Z_log_var\")(encoder_inputs)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "\n",
        "  encoder = keras.Model([encoder_inputs], [z_mean, z_log_var, z], name=\"Encoder\")\n",
        "\n",
        "  return encoder\n",
        "\n",
        "def Decoder_VAE(arquitetura,output_dim):\n",
        "  \n",
        "  latent_inputs = keras.Input(shape=(arquitetura[(len(arquitetura)-1)],),name='Entrada_Decoder')\n",
        "\n",
        "  if len(arquitetura) == 3:\n",
        "    camada1 = Dense(arquitetura[1], activation=\"linear\")(latent_inputs) # camada 1\n",
        "\n",
        "    camada2 = Dense(arquitetura[0], activation=\"linear\")(camada1) # camada 2\n",
        "    \n",
        "    decoder_outputs = Dense(output_dim, activation=\"linear\")(camada2) #  saida\n",
        "\n",
        "  if len(arquitetura) == 2:\n",
        "    camada1 = Dense(arquitetura[0], activation=\"linear\")(latent_inputs) # camada 1\n",
        "\n",
        "    decoder_outputs = Dense(output_dim, activation=\"linear\")(camada1) # saida\n",
        "  \n",
        "  if len(arquitetura) == 1:\n",
        "    decoder_outputs = Dense(output_dim, activation=\"linear\")(latent_inputs) # camada 2\n",
        "  \n",
        "  decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "\n",
        "  return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94ZrdyZZfeet"
      },
      "source": [
        "def VariationalAutoencoder(arquitetura,input_dim):    \n",
        "\n",
        "  encoder = Encoder_VAE(arquitetura,input_dim)\n",
        "\n",
        "  decoder = Decoder_VAE(arquitetura,input_dim)\n",
        "          \n",
        "  vae = VAE(encoder, decoder, input_dim)\n",
        "\n",
        "  vae.compile(optimizer=keras.optimizers.Adam())\n",
        "\n",
        "  return vae, encoder, decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywd8iy3Ad96K"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR-bOZNEfzQe"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "epocas = [5,10,50]\n",
        "\n",
        "arquiteturas = [[384,256],   # entrada tem tamanho do vetor da word-embedding\n",
        "                [256,128],   # aqui são definidas a segunda camada e/ou a camada codificada\n",
        "                [256]\n",
        "                ] \n",
        "\n",
        "caminho_resultado = './resultados/'\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "caminho = './modelos/VAE/' # caminho para salvar os modelos de rede neurais aprendidos\n",
        "\n",
        "\n",
        "word_embeddings = ['DistilBERT Multilingua']\n",
        "  \n",
        "for base in tqdm(bases.keys()):\n",
        "  df = bases[base]\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  for word_embedding in tqdm(word_embeddings):\n",
        "    print('Na word-embedding: ' + str(word_embedding))\n",
        "\n",
        "    df_outlier = np.array(df_out[word_embedding].to_list())\n",
        "\n",
        "    for percent in tqdm(percents):\n",
        "      print('No percentual: ' + str(percent))\n",
        "    \n",
        "      for epoca in tqdm(epocas):\n",
        "\n",
        "        for arq in tqdm(arquiteturas):\n",
        "            \n",
        "          name_arq = str(arq).replace('[', '(')\n",
        "          name_arq = name_arq.replace(']',')')\n",
        "\n",
        "          lista_reps = []\n",
        "          \n",
        "          for fold in tqdm(folds):\n",
        "            print('No fold ' + str(fold))\n",
        "\n",
        "            f1 = fold\n",
        "            f2 = (fold+1) % len(folds)\n",
        "            f3 = (fold+2) % len(folds)\n",
        "            \n",
        "            parcial_name = base + '_' + word_embedding + '_' + str(epoca) + '_' + str(name_arq) \n",
        "            \n",
        "            df_train,df_test,nome_arq = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3,True,parcial_name)\n",
        "            \n",
        "            vae,encoder,decoder = VariationalAutoencoder(arq,len(df_train[0]))\n",
        "\n",
        "            vae.fit(df_train, df_train, epochs=epoca, batch_size=32, verbose=0)\n",
        "            \n",
        "            save_model(encoder,caminho + nome_arq + '_encoder')\n",
        "            save_model(decoder,caminho + nome_arq + '_decoder')\n",
        "\n",
        "            # para carregar = \"load_model('path/to/location')\"\"\n",
        "\n",
        "            X_train,_,_ = encoder.predict(df_train)\n",
        "            X_test,_,_ = encoder.predict(df_test)\n",
        "            X_outlier,_,_ = encoder.predict(df_outlier)\n",
        "            \n",
        "            reps = (X_train,X_test,X_outlier)\n",
        "\n",
        "            lista_reps.append(reps)\n",
        "\n",
        "            del df_train\n",
        "            del df_test\n",
        "            del X_train\n",
        "            del X_test\n",
        "            del X_outlier\n",
        "            collect()\n",
        "\n",
        "          for modelo in tqdm(modelos):\n",
        "            print('No modelo: ' + modelo)\n",
        "\n",
        "            metricas = init_metrics()\n",
        "\n",
        "            for reps in lista_reps:\n",
        "              start = time.time()\n",
        "              values,fpr,tpr = evaluate_model(reps[0], reps[1], reps[2], modelos[modelo])\n",
        "              end = time.time()\n",
        "              temp = end - start\n",
        "              \n",
        "              values['tempo'] = temp\n",
        "\n",
        "              save_values(metricas,values)\n",
        "            \n",
        "            nome_arquivo = base + '_VAE_' + word_embedding + '_' + modelo.split('_')[0] + '_' + str(percent) + '.csv'\n",
        "\n",
        "            parametros_linha = modelo + '_' + str(epoca) + '_' + str(arq)\n",
        "            \n",
        "            escreve(metricas,nome_arquivo,parametros_linha,len(folds), caminho_resultado)\n",
        "\n",
        "          del lista_reps\n",
        "          collect()\n",
        "\n",
        "    del df_outlier\n",
        "    collect()\n",
        "\n",
        "  del df\n",
        "  del df_int\n",
        "  del df_out\n",
        "  collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxq4LgYCeA3b"
      },
      "source": [
        "## TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHWzcNUCmMSd"
      },
      "source": [
        "dic = {\n",
        "    'fcn' : { 3 : (10,[384, 256]),\n",
        "              5 : (10,[384, 256]),\n",
        "              7 : (5,[384, 256]),\n",
        "              10 : (5,[384, 256]),\n",
        "    },\n",
        "    'fnn' : { 3 : (10,[384, 256]),\n",
        "              5 : (10,[384, 256]),\n",
        "              7 : (10,[384, 256]),\n",
        "              10 : (5,[384, 256]),\n",
        "    },\n",
        "    'fakebr' : { 3 : (5,[384, 256]),\n",
        "              5 : (5,[384, 256]),\n",
        "              7 : (5,[384, 256]),\n",
        "              10 : (5,[384, 256]),\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "word_embedding = 'DistilBERT Multilingua'\n",
        "  \n",
        "for base in tqdm(bases.keys()):\n",
        "  df = bases[base]\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_outlier = np.array(df_out[word_embedding].to_list())\n",
        "\n",
        "  for percent in tqdm(percents):\n",
        "    print('No percentual: ' + str(percent))\n",
        "\n",
        "    epoca = dic[base][percent][0]\n",
        "    arq = dic[base][percent][1]\n",
        "\n",
        "    for fold in tqdm(folds):\n",
        "      print('No fold ' + str(fold))\n",
        "\n",
        "      f1 = fold\n",
        "      f2 = (fold+1) % len(folds)\n",
        "      f3 = (fold+2) % len(folds)\n",
        "\n",
        "      df_train,df_test = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3, False)\n",
        "      \n",
        "      vae,encoder,decoder = VariationalAutoencoder(arq,len(df_train[0]))\n",
        "\n",
        "      vae.fit(df_train, df_train, epochs=epoca, batch_size=32, verbose=0)\n",
        "\n",
        "      X_train,_,_ = encoder.predict(df_train)\n",
        "      X_test,_,_ = encoder.predict(df_test)\n",
        "      X_outlier,_,_ = encoder.predict(df_outlier)\n",
        "\n",
        "      name = 'VAE/' + base + '/' + str(percent) + '/tsne_fold' + str(f1) \n",
        "\n",
        "      plot_TSNE(np.concatenate((X_train,X_test)), X_outlier, name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nFTSCPjWjrM"
      },
      "source": [
        "# Multimodal Variational Autoencoder (MVAE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv_fUkZUeHXi"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvTTaS-KwTbu"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Input, concatenate, multiply, average, subtract, add\n",
        "from tensorflow.keras.models import Model, load_model, save_model\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "  \"\"\"Uses (z_mean, z_log_var) to sample z.\"\"\"\n",
        "\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class MVAE(keras.Model):\n",
        "  def __init__(self, encoder, decoder, factor_multiply_embedding ,factor_multiply_density,  **kwargs):\n",
        "    super(MVAE, self).__init__(**kwargs)\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.factor_multiply_embedding = factor_multiply_embedding\n",
        "    self.factor_multiply_density = factor_multiply_density\n",
        "\n",
        "  def train_step(self, data):\n",
        "\n",
        "    if isinstance(data, tuple):\n",
        "      data = data[0]\n",
        "    with tf.GradientTape() as tape:\n",
        "      \n",
        "      z_mean, z_log_var, z = self.encoder((data[0],data[1]))\n",
        "\n",
        "      reconstruction = self.decoder(z)\n",
        "\n",
        "      embedding_loss = tf.reduce_mean(\n",
        "          keras.losses.mean_squared_error(data[0], reconstruction[0])\n",
        "      )\n",
        "      \n",
        "      embedding_loss *= self.factor_multiply_embedding\n",
        "\n",
        "      density_loss = tf.reduce_mean(\n",
        "          keras.losses.mean_squared_error(data[1], reconstruction[1])\n",
        "      )\n",
        "      \n",
        "      density_loss *= self.factor_multiply_density\n",
        "\n",
        "      kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "      kl_loss = tf.reduce_mean(kl_loss)\n",
        "      kl_loss *= -0.5\n",
        "      total_loss = embedding_loss + density_loss + kl_loss\n",
        "\n",
        "\n",
        "    grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "    return {\n",
        "      \"tota loss\": total_loss,\n",
        "      \"embedding loss\": embedding_loss,\n",
        "      \"denisty loss\": density_loss,\n",
        "      \"kl loss\": kl_loss,\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg05HVSg0Pn7"
      },
      "source": [
        "def Encoder_MVAE(arquitetura,embedding_dim, density_dim, operator):\n",
        "\n",
        "  embedding_inputs = keras.Input(shape=(embedding_dim,), name='Entrada_Embedding_Encoder')\n",
        "  density_inputs = keras.Input(shape=(density_dim,), name='Entrada_Density_Encoder') \n",
        "\n",
        "  l1 = Dense(np.max([embedding_dim,density_dim]), activation='linear')(embedding_inputs)\n",
        "  l2 = Dense(np.max([embedding_dim,density_dim]), activation='linear')(density_inputs)\n",
        "\n",
        "  fusion = None\n",
        "  if operator=='concatenate': fusion = concatenate([l1,l2])\n",
        "  if operator=='multiply': fusion = multiply([l1,l2])\n",
        "  if operator=='average': fusion = average([l1,l2])\n",
        "  if operator=='subtract': fusion = subtract([l1,l2])\n",
        "  if operator=='add': fusion = add([l1,l2])\n",
        "\n",
        "  \n",
        "  if len(arquitetura) == 3:\n",
        "    camada1 = Dense(arquitetura[0], activation=\"linear\")(fusion) # camada 1\n",
        "\n",
        "    camada2 = Dense(arquitetura[1], activation=\"linear\")(camada1) # camada 2\n",
        "    \n",
        "    z_mean = layers.Dense(arquitetura[2], name=\"Z_mean\")(camada2)\n",
        "    z_log_var = layers.Dense(arquitetura[2], name=\"Z_log_var\")(camada2)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "  if len(arquitetura) == 2:\n",
        "    camada1 = Dense(arquitetura[0], activation=\"linear\")(fusion) # camada 1\n",
        "    \n",
        "    z_mean = layers.Dense(arquitetura[1], name=\"Z_mean\")(camada1)\n",
        "    z_log_var = layers.Dense(arquitetura[1], name=\"Z_log_var\")(camada1)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "  \n",
        "  if len(arquitetura) == 1:\n",
        "    z_mean = layers.Dense(arquitetura[0], name=\"Z_mean\")(fusion)\n",
        "    z_log_var = layers.Dense(arquitetura[0], name=\"Z_log_var\")(fusion)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "\n",
        "  encoder = keras.Model([embedding_inputs,density_inputs], [z_mean, z_log_var, z], name=\"Encoder\")\n",
        "\n",
        "  return encoder\n",
        "\n",
        "def Decoder_MVAE(arquitetura, embedding_dim, density_dim):\n",
        "  \n",
        "  latent_inputs = keras.Input(shape=(arquitetura[(len(arquitetura)-1)],),name='Entrada_Decoder')\n",
        "\n",
        "  if len(arquitetura) == 3:\n",
        "    camada1 = Dense(arquitetura[1], activation=\"linear\")(latent_inputs) # camada 1\n",
        "\n",
        "    camada2 = Dense(arquitetura[0], activation=\"linear\")(camada1) # camada 2\n",
        "    \n",
        "    embedding_outputs = Dense(embedding_dim, activation=\"linear\")(camada2) #  saida embedding\n",
        "\n",
        "    density_outputs = Dense(density_dim, activation=\"linear\")(camada2) #  saida density\n",
        "\n",
        "  if len(arquitetura) == 2:\n",
        "    camada1 = Dense(arquitetura[0], activation=\"linear\")(latent_inputs) # camada 1\n",
        "\n",
        "    embedding_outputs = Dense(embedding_dim, activation=\"linear\")(camada1) #  saida embedding\n",
        "\n",
        "    density_outputs = Dense(density_dim, activation=\"linear\")(camada1) #  saida density\n",
        "  \n",
        "  if len(arquitetura) == 1:\n",
        "    embedding_outputs = Dense(embedding_dim, activation=\"linear\")(latent_inputs) #  saida embedding\n",
        "\n",
        "    density_outputs = Dense(density_dim, activation=\"linear\")(latent_inputs) #  saida density\n",
        "  \n",
        "  decoder = keras.Model(latent_inputs, [embedding_outputs,density_outputs], name=\"decoder\")\n",
        "\n",
        "  return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKgeQv4v6NDg"
      },
      "source": [
        "def MultimodalVAE(arquitetura,embedding_dim,density_dim,operator):    \n",
        "\n",
        "  encoder = Encoder_MVAE(arquitetura, embedding_dim, density_dim, operator)\n",
        "\n",
        "  decoder = Decoder_MVAE(arquitetura, embedding_dim, density_dim)\n",
        "          \n",
        "  mvae = MVAE(encoder, decoder, embedding_dim, density_dim)\n",
        "\n",
        "  mvae.compile(optimizer=keras.optimizers.Adam())\n",
        "\n",
        "  return mvae, encoder, decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi74MDq-r9RJ"
      },
      "source": [
        "## Evaluation MVAE-FakeNews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXsec3kbZ8c8"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "epocas = [5,10,50]\n",
        "\n",
        "arquiteturas = [[384,256],   # entrada tem tamanho do vetor da word-embedding\n",
        "                [256,128],   # aqui são definidas a segunda camada e/ou a camada codificada\n",
        "                [256]\n",
        "                ] \n",
        "\n",
        "caminho_resultado = './resultados/'\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "matriz_grupos = [[2,4,6,8,10],\n",
        "                 [3,5,7,9],\n",
        "                 [2,3,4,5,6,7,8,9,10]\n",
        "                ] \n",
        "\n",
        "operadores = ['concatenate','multiply','average','subtract','add']\n",
        "\n",
        "caminho = './modelos/MVAE/' # caminho para salvar os modelos de rede neurais aprendidos\n",
        "\n",
        "word_embeddings = ['DistilBERT Multilingua']\n",
        "  \n",
        "for base in tqdm(bases.keys()):\n",
        "  df = bases[base]\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  for word_embedding in tqdm(word_embeddings):\n",
        "    print('Na word-embedding: ' + str(word_embedding))\n",
        "\n",
        "    df_outlier = np.array(df_out[word_embedding].to_list())\n",
        "\n",
        "    for percent in tqdm(percents):\n",
        "    \n",
        "        for epoca in tqdm(epocas):\n",
        "\n",
        "            for arq in tqdm(arquiteturas):\n",
        "                \n",
        "                name_arq = str(arq).replace('[', '(')\n",
        "                name_arq = name_arq.replace(']',')')\n",
        "\n",
        "                for lista_grupos in tqdm(matriz_grupos):\n",
        "                    \n",
        "                    name_lista_grupos = str(lista_grupos).replace('[', '(')\n",
        "                    name_lista_grupos = name_lista_grupos.replace(']', ')')\n",
        "\n",
        "                    for operador in tqdm(operadores):\n",
        "\n",
        "                        lista_reps = []\n",
        "\n",
        "                        for fold in tqdm(folds):\n",
        "\n",
        "                            f1 = fold\n",
        "                            f2 = (fold+1) % len(folds)\n",
        "                            f3 = (fold+2) % len(folds)\n",
        "\n",
        "                            parcial_name = base + '_' + word_embedding + '_' + str(epoca) + '_' + str(name_arq) + '_' + str(name_lista_grupos) + '_' + str(operador)\n",
        "                            \n",
        "                            df_train,df_test,nome_arq = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3,True,parcial_name)\n",
        "\n",
        "                            denisty_train, density_test, density_outlier = informacao_de_densidade(lista_grupos,df_train,df_test,df_outlier)\n",
        "\n",
        "                            mvae, encoder, decoder = MultimodalVAE(arq,len(df_train[0]),len(lista_grupos),operador)\n",
        "\n",
        "                            mvae.fit([df_train,denisty_train], [df_train,denisty_train], epochs=epoca, batch_size=32, verbose=0)\n",
        "\n",
        "                            save_model(encoder,caminho + nome_arq + '_encoder')\n",
        "                            save_model(decoder,caminho + nome_arq + '_decoder')\n",
        "\n",
        "                            # para carregar = \"load_model('path/to/location')\"\"\n",
        "\n",
        "                            X_train,_,_ = encoder.predict([df_train,denisty_train])\n",
        "                            X_test,_,_ = encoder.predict([df_test,density_test])\n",
        "                            X_outlier,_,_ = encoder.predict([df_outlier,density_outlier])\n",
        "\n",
        "                            reps = (X_train,X_test,X_outlier)\n",
        "\n",
        "                            lista_reps.append(reps)\n",
        "                            \n",
        "                            del df_train\n",
        "                            del df_test\n",
        "                            del X_train\n",
        "                            del X_test\n",
        "                            del X_outlier\n",
        "                            collect()\n",
        "\n",
        "                        \n",
        "                        for modelo in tqdm(list(modelos.keys())):\n",
        "\n",
        "                            metricas = init_metrics()\n",
        "                                \n",
        "                            for reps in lista_reps:    \n",
        "                                start = time.time()\n",
        "                                values,fpr,tpr = evaluate_model(reps[0], reps[1], reps[2], modelos[modelo])\n",
        "                                end = time.time()\n",
        "                                temp = end - start\n",
        "                                \n",
        "                                values['tempo'] = temp\n",
        "\n",
        "                                save_values(metricas,values)\n",
        "\n",
        "                            nome_arquivo = base + '_MVAE_' + word_embedding + '_' + modelo.split('_')[0] + '_' + str(percent) + '.csv'\n",
        "\n",
        "                            parametros_linha = modelo + '_' + str(epoca) + '_' + str(arq) + '_' + str(lista_grupos) + '_' + str(operador)\n",
        "\n",
        "                            escreve(metricas,nome_arquivo,parametros_linha,len(folds), caminho_resultado)\n",
        "                        \n",
        "                        del lista_reps\n",
        "                        collect()                  \n",
        "\n",
        "    del df_outlier\n",
        "    collect()\n",
        "\n",
        "  del df\n",
        "  del df_int\n",
        "  del df_out\n",
        "  collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12-_jXscf3pn"
      },
      "source": [
        "## TSNE MVAE-FakeNews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rcMUBqlf_uW"
      },
      "source": [
        "dic = {\n",
        "    'fcn' : { 3 : (50\t,[384, 256]\t,[3, 5, 7, 9],\t'average'),\n",
        "              5 : (50\t,[256, 128],[3, 5, 7, 9],\t'average'),\n",
        "              7 : (10,\t[384, 256],\t[3, 5, 7, 9],\t'average'),\n",
        "              10 : (10,\t[384, 256],\t[2, 4, 6, 8, 10],\t'average'),\n",
        "    },\n",
        "    'fnn' : { 3 : (50,[256, 128],\t[2, 3, 4, 5, 6, 7, 8, 9, 10],\t'multiply'),\n",
        "              5 : (10,\t[256, 128],\t[2, 4, 6, 8, 10],\t'multiply'),\n",
        "              7 : (5,\t[384, 256],\t[2, 4, 6, 8, 10],\t'multiply'),\n",
        "              10 : (5,\t[384, 256],\t[2, 4, 6, 8, 10],\t'multiply'),\n",
        "    },\n",
        "    'fakebr' : { 3 : (5\t,[384, 256]\t,[2, 4, 6, 8, 10],\t'multiply'), \n",
        "              5 : (10\t,[256],\t[2, 3, 4, 5, 6, 7, 8, 9, 10],\t'multiply'),\n",
        "              7 : (10\t,[256],\t[2, 3, 4, 5, 6, 7, 8, 9, 10],\t'multiply'),\n",
        "              10 : (10,\t[256],\t[2, 3, 4, 5, 6, 7, 8, 9, 10],\t'multiply'),\n",
        "    },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnUDiQAiC5P"
      },
      "source": [
        "word_embedding = 'DistilBERT Multilingua'\n",
        "  \n",
        "for base in tqdm(bases.keys()):\n",
        "  df = bases[base]\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_outlier = np.array(df_out[word_embedding].to_list())\n",
        "\n",
        "  for percent in tqdm(percents):\n",
        "\n",
        "    epoca = dic[base][percent][0]\n",
        "    arq = dic[base][percent][1]\n",
        "    operador = dic[base][percent][3]\n",
        "    lista_grupos = dic[base][percent][2]\n",
        "\n",
        "    for fold in tqdm(folds):\n",
        "\n",
        "      f1 = fold\n",
        "      f2 = (fold+1) % len(folds)\n",
        "      f3 = (fold+2) % len(folds)\n",
        "      \n",
        "      df_train,df_test = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3,False)\n",
        "\n",
        "      denisty_train, density_test, density_outlier = informacao_de_densidade(lista_grupos,df_train,df_test,df_outlier)\n",
        "\n",
        "      mvae, encoder, decoder = MultimodalVAE(arq,len(df_train[0]),len(lista_grupos),operador)\n",
        "\n",
        "      mvae.fit([df_train,denisty_train], [df_train,denisty_train], epochs=epoca, batch_size=32, verbose=0)\n",
        "\n",
        "      X_train,_,_ = encoder.predict([df_train,denisty_train])\n",
        "      X_test,_,_ = encoder.predict([df_test,density_test])\n",
        "      X_outlier,_,_ = encoder.predict([df_outlier,density_outlier])\n",
        "\n",
        "      name = 'MVAE/' + base + '/' + str(percent) + '/tsne_fold' + str(f1) \n",
        "\n",
        "      plot_TSNE(np.concatenate((X_train,X_test)), X_outlier, name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr3B1b17sEDR"
      },
      "source": [
        "## Evalaution MVAE-LIWK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtIbXwD-sLkD"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "epocas = [5,10,50]\n",
        "\n",
        "arquiteturas = [[384,256],   # entrada tem tamanho do vetor da word-embedding\n",
        "                [256,128],   # aqui são definidas a segunda camada e/ou a camada codificada\n",
        "                [256]\n",
        "                ] \n",
        "\n",
        "caminho_resultado = './resultados/'\n",
        "\n",
        "percents = [3,5,7,10]\n",
        "\n",
        "operadores = ['concatenate','multiply','average','subtract','add']\n",
        "\n",
        "caminho = './modelos/MVAE_golo_mari/' # caminho para salvar os modelos de rede neurais aprendidos\n",
        "\n",
        "word_embeddings = ['DistilBERT Multilingua']\n",
        "  \n",
        "for base in tqdm(bases.keys()):\n",
        "  df = bases[base]\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  for word_embedding in tqdm(word_embeddings):\n",
        "\n",
        "    df_outlier_emb = np.array(df_out[word_embedding].to_list())\n",
        "    df_outlier_fet = np.array(df_out['features_normalized'].to_list()).astype('float32')\n",
        "\n",
        "    for percent in tqdm(percents):\n",
        "    \n",
        "        for epoca in tqdm(epocas):\n",
        "\n",
        "            for arq in tqdm(arquiteturas):\n",
        "                \n",
        "                name_arq = str(arq).replace('[', '(')\n",
        "                name_arq = name_arq.replace(']',')')\n",
        "\n",
        "                for operador in tqdm(operadores):\n",
        "\n",
        "                    lista_reps = []\n",
        "\n",
        "                    for fold in tqdm(folds):\n",
        "\n",
        "                        f1 = fold\n",
        "                        f2 = (fold+1) % len(folds)\n",
        "                        f3 = (fold+2) % len(folds)\n",
        "\n",
        "                        parcial_name = base + '_' + word_embedding + '_' + str(epoca) + '_' + str(name_arq) + '_' + str(operador)\n",
        "                        \n",
        "                        df_train_emb,df_test_emb,nome_arq = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3,True,parcial_name)\n",
        "                        df_train_fet,df_test_fet = train_test_split_pipeline(df_int, percent,'features_normalized', f1,f2,f3,False)\n",
        "\n",
        "                        df_train_fet = df_train_fet.astype('float32')\n",
        "                        df_test_fet = df_test_fet.astype('float32')\n",
        "\n",
        "                        mvae, encoder, decoder = MultimodalVAE(arq,len(df_train_emb[0]),len(df_train_fet[0]),operador)\n",
        "\n",
        "                        mvae.fit([df_train_emb,df_train_fet], [df_train_emb,df_train_fet], epochs=epoca, batch_size=32, verbose=0)\n",
        "\n",
        "                        save_model(encoder,caminho + nome_arq + '_encoder')\n",
        "                        save_model(decoder,caminho + nome_arq + '_decoder')\n",
        "\n",
        "                        # para carregar = \"load_model('path/to/location')\"\"\n",
        "\n",
        "                        X_train,_,_ = encoder.predict([df_train_emb,df_train_fet])\n",
        "                        X_test,_,_ = encoder.predict([df_test_emb,df_test_fet])\n",
        "                        X_outlier,_,_ = encoder.predict([df_outlier_emb,df_outlier_fet])\n",
        "\n",
        "                        reps = (X_train,X_test,X_outlier)\n",
        "\n",
        "                        lista_reps.append(reps)\n",
        "\n",
        "                        del df_train_emb\n",
        "                        del df_test_emb\n",
        "                        del df_train_fet\n",
        "                        del df_test_fet\n",
        "                        del X_train\n",
        "                        del X_test\n",
        "                        del X_outlier\n",
        "                        collect()\n",
        "                    \n",
        "                    for modelo in tqdm(list(modelos.keys())):\n",
        "\n",
        "                        metricas = init_metrics()\n",
        "                            \n",
        "                        for reps in lista_reps:\n",
        "\n",
        "                            start = time.time()\n",
        "                            values,fpr,tpr = evaluate_model(reps[0], reps[1], reps[2], modelos[modelo])\n",
        "                            end = time.time()\n",
        "                            temp = end - start\n",
        "                            values['tempo'] = temp\n",
        "                            save_values(metricas,values)\n",
        "\n",
        "                        nome_arquivo = base + '_MVAE-GM_' + word_embedding + '_' + modelo.split('_')[0] + '_' + str(percent) + '.csv'\n",
        "\n",
        "                        parametros_linha = modelo + '_' + str(epoca) + '_' + str(arq) + '_' + str(operador)\n",
        "\n",
        "                        escreve(metricas,nome_arquivo,parametros_linha,len(folds), caminho_resultado)\n",
        "                    del lista_reps\n",
        "                    collect()\n",
        "\n",
        "    del df_outlier_emb\n",
        "    del df_outlier_fet\n",
        "    collect()\n",
        "\n",
        "  del df\n",
        "  del df_int\n",
        "  del df_out\n",
        "  collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hbkP9qDgBEn"
      },
      "source": [
        "## TSNE MVAE-LIWK\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T7UMtumiIAI"
      },
      "source": [
        "dic = {\n",
        "    'fcn' : { 3 : (50\t,\t[256, 128],\t'average'),\n",
        "              5 : (50\t,\t[384, 256],\t'average'),\n",
        "              7 : (50\t,\t[384, 256],\t'average'),\n",
        "              10 : (50,\t\t[384, 256],\t'average'),\n",
        "    },\n",
        "    'fnn' : { 3 : (10\t,\t[256]\t,'subtract'),\n",
        "              5 : (10\t,\t[384, 256]\t,'subtract'),\n",
        "              7 : (50\t,\t[384, 256],\t'add'),\n",
        "              10 : (10,\t\t[384, 256],\t'subtract'),\n",
        "    },\n",
        "    'fakebr' : { 3 : (5\t,\t[384, 256],\t'average'), \n",
        "              5 : (5\t,\t[384, 256]\t,'average'),\n",
        "              7 : (50\t,\t[384, 256],\t'average'),\n",
        "              10 : (50\t,\t[384, 256],\t'average'),\n",
        "    },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "259ECCm7iNlz"
      },
      "source": [
        "word_embedding = 'DistilBERT Multilingua'\n",
        "  \n",
        "for base in tqdm(bases.keys()):\n",
        "  df = bases[base]\n",
        "\n",
        "  df_int = df[df['class'] == 1]\n",
        "\n",
        "  df_out = df[df['class'] == -1]\n",
        "\n",
        "  folds = df_int['fold'].unique()\n",
        "\n",
        "  df_outlier_emb = np.array(df_out[word_embedding].to_list())\n",
        "  df_outlier_fet = np.array(df_out['features_normalized'].to_list()).astype('float32')\n",
        "\n",
        "  for percent in tqdm(percents):\n",
        "\n",
        "    epoca = dic[base][percent][0]\n",
        "    arq = dic[base][percent][1]\n",
        "    operador = dic[base][percent][2]\n",
        "\n",
        "    for fold in tqdm(folds):\n",
        "\n",
        "      f1 = fold\n",
        "      f2 = (fold+1) % len(folds)\n",
        "      f3 = (fold+2) % len(folds)\n",
        "      \n",
        "      df_train_emb,df_test_emb = train_test_split_pipeline(df_int, percent,word_embedding, f1,f2,f3,False)\n",
        "      df_train_fet,df_test_fet = train_test_split_pipeline(df_int, percent,'features_normalized', f1,f2,f3,False)\n",
        "\n",
        "      df_train_fet = df_train_fet.astype('float32')\n",
        "      df_test_fet = df_test_fet.astype('float32')\n",
        "\n",
        "      mvae, encoder, decoder = MultimodalVAE(arq,len(df_train_emb[0]),len(df_train_fet[0]),operador)\n",
        "\n",
        "      mvae.fit([df_train_emb,df_train_fet], [df_train_emb,df_train_fet], epochs=epoca, batch_size=32, verbose=0)\n",
        "\n",
        "      X_train,_,_ = encoder.predict([df_train_emb,df_train_fet])\n",
        "      X_test,_,_ = encoder.predict([df_test_emb,df_test_fet])\n",
        "      X_outlier,_,_ = encoder.predict([df_outlier_emb,df_outlier_fet])\n",
        "\n",
        "\n",
        "      name = 'MVAE-LIWK/' + base + '/' + str(percent) + '/tsne_fold' + str(f1) \n",
        "\n",
        "      plot_TSNE(np.concatenate((X_train,X_test)), X_outlier, name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}